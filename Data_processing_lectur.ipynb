{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_processing_lectur.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3czFFWOhAHkocpqmg609g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lio-cmr/-python-practicals-/blob/main/Data_processing_lectur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "juuEBpMfXbsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we use an algorithm to fit our data it assumes that the data is centered and the order of variance of all features are the same otherwise the estimators will not predict correctly.\n",
        "\n",
        "The sklearn library has a method to standardize the data set with StandardScaler in preprocessing class.\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ9vD41TXprD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "_q5dllA0W2xo",
        "outputId": "91df5be7-fc76-4adb-d6f0-81d1be1b7c2d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-60cc9084e3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "#Before modeling our estimator we should always some preprocessing scaling.\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling with sparse data and outliers"
      ],
      "metadata": {
        "id": "N8oF8uw-YI9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Scaling with Sparse data:"
      ],
      "metadata": {
        "id": "GRlVWHgzYbh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling of data is another way of making feature values be in some range of “0” and “1”. There are two methods of doing these i.e. MinMaxScaler and MaxAbsScaler.\n",
        "\n",
        "Example with python"
      ],
      "metadata": {
        "id": "HIEjxtq7Ye7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_train = np.array([[ 1., 0.,  2.], [ 2.,  0.,  -1.], [ 0.,  2.,\n",
        "                                                             -1.]])\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
        "print(X_train_minmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMAZzpDX7-j",
        "outputId": "91539a57-cd2e-44c8-be4b-094998d64dd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5 0.  1. ]\n",
            " [1.  0.  0. ]\n",
            " [0.  1.  0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-Scaling with Outliers:"
      ],
      "metadata": {
        "id": "fnKVkOyBeJWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When raw data have many outliers then the scaling with mean and variance doesn’t do well with the data. So, we have to use a more robust method like the interquartile method (IQR) because the outliers are influenced by mean and variance. The range of the IQR is between 25% and 75% in which the median is removed and scaling the quantile range.\n",
        "\n",
        "The RobustScaler takes some parameters to perform scaling.\n",
        "\n",
        "\n",
        "-The first parameter is with_centering that centers the data before scaling if it is true.\n",
        "\n",
        "-The second parameter is with_scaling if it is true then it scale the data in the quantile range.\n",
        "\n",
        "\n",
        "Example with python"
      ],
      "metadata": {
        "id": "1Aj7E3lYeNFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "X = [[ 1., 0.,  2.], [ 2.,  0.,  -1.], [ 0.,  2., -1.]]\n",
        "transformer = RobustScaler().fit(X)\n",
        "\n",
        "transformer.transform(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd9E7N_lZeSD",
        "outputId": "4ab44832-e1bf-435f-d2f3-649e9275cd4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  2.],\n",
              "       [ 1.,  0.,  0.],\n",
              "       [-1.,  2.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization"
      ],
      "metadata": {
        "id": "5kex2liDg3tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scaling process in this is to normalize the values to their unit norm. An example of this normalization is MinMaxScaler. The process is useful when we are dealing with quadratic form in pair forms it can be kernel-based or dot product-based.\n",
        "\n",
        "It is also useful based on of vector space model i.e the vectors related with text data samples to ease in data filtration.\n",
        "\n",
        "Two types of Normalization happen as shown below:\n",
        "\n",
        "- Normalize: It deals to scale the input vectors to unit norm. The norm parameter is used to normalize all the non-zero values. It takes three arguments L1, L2, and max where the L2 is the default norm.\n",
        "\n",
        "- Normalizer: It also does the same operation but in this process the fit method is optional.\n",
        "\n",
        "Example with Python:\n",
        "\n"
      ],
      "metadata": {
        "id": "ut8kpmwHhDGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "X = [[ 1., 0., 2.], [ 2., 0., -1.], [ 0., 2., -1.]]\n",
        "X_normalized = normalize(X, norm='l2')\n",
        "print(X_normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY-Ca475grKI",
        "outputId": "800da83a-944a-4cde-c6c2-e408b98e802d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.4472136   0.          0.89442719]\n",
            " [ 0.89442719  0.         -0.4472136 ]\n",
            " [ 0.          0.89442719 -0.4472136 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with Normalizer:"
      ],
      "metadata": {
        "id": "JlpkZ16nwXHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalizer is useful in the pipeline of data processing in the beginning.\n",
        "\n",
        "When we use sparse input it is important to convert it not CSR format to avoid multiple memory copies. The CSR is compressed Sparse Rows comes in scipy.sparse.csr_matrix."
      ],
      "metadata": {
        "id": "GxOj4QrGw8Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Encoding"
      ],
      "metadata": {
        "id": "FpWQf0OByZ5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we get some raw data set then some columns are that are not in continuous values rather in some categories of binary and multiple categories. So, to make them in integer value we use encoding methods. There are some encoding methods given below:\n",
        "\n",
        "- **Get Dummies:** It is used to get a new feature column with 0 and 1 encoding the categories with the help of the pandas’ library.\n",
        "\n",
        "- **Label Encoder:** It is used to encode binary categories to numeric values in the sklearn library.\n",
        "\n",
        "- **One Hot Encoder:** The sklearn library provides another feature to convert categories class to new numeric values of 0 and 1 with new feature columns.\n",
        "\n",
        "- **Hashing:** It is more useful than one-hot encoding in the case of high dimensions. It is used when there is high cardinality in the feature."
      ],
      "metadata": {
        "id": "wl1V-VPoyf14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many other encoding methods like **mean encoding, Helmert encoding, ordinal encoding**, probability ratio encoding and, etc.\n",
        "\n",
        "Example with Python:"
      ],
      "metadata": {
        "id": "5OMcbF50zbGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1=pd.get_dummies(df['State'],drop_first=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "ZQYAXwU2w0iL",
        "outputId": "84028c6f-1f39-4de2-ea01-1654ceedf731"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d0ac1c504bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'State'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation"
      ],
      "metadata": {
        "id": "kag5m_Ie1u_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when raw data have some missing values so to make the missing record to a numeric value is know as imputing.\n",
        "\n",
        "Creating the random data frame.\n",
        "\n"
      ],
      "metadata": {
        "id": "pxLjBUm71yv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the pandas library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.DataFrame(np.random.randn(4, 3), index=['a', 'c', 'e',\n",
        "'h'],columns=['First', 'Second', 'Three'])\n",
        "\n",
        "df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV1nVC5jz0Kc",
        "outputId": "367abb7e-3db7-4eaa-f080-5fd57f027f8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      First    Second     Three\n",
            "a -0.857874  0.914767  0.113066\n",
            "b       NaN       NaN       NaN\n",
            "c  0.671168 -0.706969 -0.362725\n",
            "d       NaN       NaN       NaN\n",
            "e  1.111266 -0.171088  0.257602\n",
            "f       NaN       NaN       NaN\n",
            "g       NaN       NaN       NaN\n",
            "h -1.803207  0.442516  0.939384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now replacing with zero value."
      ],
      "metadata": {
        "id": "x8dwXG2Q2ws6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"NaN replaced with '0':\")\n",
        "print (df.fillna(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWWX218Y2fmk",
        "outputId": "2b09e949-b8c8-43a8-fe85-86ef4dd027b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN replaced with '0':\n",
            "      First    Second     Three\n",
            "a -0.857874  0.914767  0.113066\n",
            "b  0.000000  0.000000  0.000000\n",
            "c  0.671168 -0.706969 -0.362725\n",
            "d  0.000000  0.000000  0.000000\n",
            "e  1.111266 -0.171088  0.257602\n",
            "f  0.000000  0.000000  0.000000\n",
            "g  0.000000  0.000000  0.000000\n",
            "h -1.803207  0.442516  0.939384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the missing values with mean."
      ],
      "metadata": {
        "id": "XQYMEGjs28I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n"
      ],
      "metadata": {
        "id": "2JIGlR7G21SH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sklearn provide simple imputer to find the NAN values and fill with mean.\n",
        "\n",
        "We can use imputer in the pipeline to make an estimator better."
      ],
      "metadata": {
        "id": "EAkEB8HP3UVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "jstiVSec3aPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data preprocessing is an important step to perform to make the data set more reliable to our estimators."
      ],
      "metadata": {
        "id": "2dcu3cdY3jDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_4doq6YF3AHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}